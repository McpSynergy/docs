# 服务端介绍

欢迎来到 McpSynergy Server。它是一个实现了模型上下文协议（MCP）的后端服务，充当着连接大型语言模型（LLM，如 OpenAI 的 GPT 系列）和您的前端应用程序的智能桥梁。

它的核心任务是：将用户的自然语言请求，转化为前端可以渲染的、丰富的、可交互的 UI 组件指令。

---

## 核心价值

> **赋能 AI**
> 让 AI 不再局限于文本交流。通过为 AI 提供一系列“工具”（即前端 UI 组件），服务器使得 AI 能够决定何时需要通过更丰富的组件来展示信息或与用户交互。

> **业务逻辑中心**
> 服务器是执行工具背后业务逻辑的地方。当 AI 决定调用一个工具时（例如，查询用户信息），服务器负责执行实际的数据库查询、API 调用等操作，并将结果处理成前端组件所需的格式。

> **动态与灵活**
> 您可以在后端定义新的工具，或修改现有工具的行为，而无需改动客户端代码。这使得您的 AI 应用能够快速迭代和适应新的业务需求。

> **协议标准**
> 基于标准的模型上下文协议（MCP），确保了与遵循该协议的各种模型和客户端的互操作性。

## 工作流程概览

1.  **加载工具定义**: 服务器启动时，会加载一份描述所有可用前端组件（即“工具”）的 schema 文件（通常由 `client` 端生成）。
2.  **构建智能提示**: 当收到用户的聊天消息时，服务器会将用户的消息和可用的工具列表一起构建成一个系统提示（System Prompt），发送给 LLM。
3.  **解析 AI 意图**: LLM 会返回它的回答。如果它认为需要使用某个工具，那么它的回答将是一个特定格式的 JSON，指明了工具名称和所需参数。
4.  **执行工具逻辑**: 服务器解析这个 JSON，找到对应的工具处理器，并执行相关的业务逻辑（例如，从数据库获取数据）。
5.  **构建组件 Props**: 工具逻辑的执行结果被格式化成一个符合前端组件 props 要求的 JSON 对象。
6.  **发送渲染指令**: 服务器最终将工具名称和 props 数据一起发送回客户端，指示客户端渲染指定的 UI 组件。

现在，继续阅读 [核心概念](./concepts.md) 来更深入地理解其工作原理。
